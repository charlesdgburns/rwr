---
title: "rwr_FastSim"
author: "Charles David Gutierrez Burns"
date: "2023-08-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggpubr)
```

## Fast simulation

In a previous analysis (slow simulation) we simulated data per participant, correlating edges and behavioural score, and resampled participants in order to closely follow Marek, Tervo-Clemmens et al.

There we show strong bias in results when there is no effect, calling for further methodological investigation when true effects are present. We refer to the following simulations as 'fast' simulations, as we can simulate effects more efficiently now that we understand how sampling variability is compounded by resampling with replacement. Instead of painstakingly simulating non-gaussian multivariate data, we can simply draw from corresponding parametric distributions. 

This allows us to comprehensively answer 'when and how much' bias arises when resampling with replacement under various true effects.

The 'slow' simulations indicated clearly that uncorrected estimates are generally biased. 
This should not be surprising, as low statistical thresholds will include many false positives when not controlling for multiple comparisons. Furthermore, the 'slow' simulations also indicated that bias of resampling at the full sample size will depend on statistical power of the effect at the full sample size. For this reason, in the next analysis we fix the statistical threshold at bonferroni corrected levels and vary statistical power.

We therefore aim to investigate a broad range of effect sizes, based on an inverse power calculation for a full sample size of n = 1,000 (as with our slow simulations).

As a final figure for these simulations, we want a graph for each statistical error which demonstrates bias (estimated - expected) for each statistical error for different levels of power at the full sample size. 

```{r initial parameters}
#### Input variables ####

#The parameters below may be scaled to lower computation time.

nDiscovery <- 1000; #sample size of our simulated data.
binsize <- c(25,33,50, 70, 100,135, 200, 265, 375, 525, 725, 1000); #resample size bins
iter <- 100 #Number of iterations
nEffects <- 55278
thr <- 0.05/nEffects #significance threshold, fixed here as power varies
power <- c(0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99)

```

We find critical effect sizes for a discovery sample of n = 1000, at a bonferroni corrected significance threshold.

```{r Analytic Power Calculations}
#First, we want to find the critical effect sizes for different levels of statistical power at the full sample size. 

rCrit <- c()

for(p in 1:length(power)){
  Zb <- qnorm(power[p]) #Critical Z score for a given power
    Za <- qnorm(1-thr/2) #Critical Z score for a given threshold
        z <- (Zb+Za)/sqrt(nDiscovery-3) #Power scaled to the distribution at full sample size
        rCrit[p] <- tanh(z) #Fisher transformation from Z dist to Pearson Correlation Dist.
  }

```

The above effect sizes will be used to simulate true effects of a given power level (at n=1000, alpha bonferonni corrected). 

Note that while real world effect sizes of a single BWAS may vary, we instead simulate several effects with the same underlying effect size. This should not be an issue, since the statistical error summary statistics are given as an average across effects, so in this simulation we can think of the underlying effects having an average effect size according to a given power level.

Below, we generate a discovery sample such that 1% of the effects are true effects, with their effect size scaled according to power levels. E.g. with 1225 effects, 12 of them are true effects. The number of true effects here is a somewhat arbitrary decision.

```{r Discovery Sample Generation}

nTrue <- round(nEffects/100)
nFalse <- nEffects - nTrue

Discovery <- list()

#initialise dataframes
Discovery$corrs <- array(0, dim=c(nEffects,length(power))) #generate a list of effects for each power
Discovery$pvals <- array(0, dim=c(nEffects,length(power))) #generate a list of pvalues for each power

for(p in 1:length(power)){

        df <- nDiscovery-2

        ts <- c() #temporary hold some sampled t statistics
          
        ts <- rt(nEffects, df) #Start by sampling all randomly.
        
        Discovery$corrs[,p] <-  ts/sqrt(df+ts^2) #Inverse function from Pearson r to t statistic.
        
        Discovery$corrs[1:nTrue,p] <- Discovery$corrs[1:nTrue,p]+rCrit[p] #True effects simulated by adding sampling variability (random effect size) to the true effect size.
        
        Discovery$pvals[,p] <-  sapply(Discovery$corrs[,p]*sqrt(df/(1-Discovery$corrs[,p]^2)) , function(x){2*min(pt(x, df), pt(x, df, lower.tail=FALSE))}) #calculating p-values

}


```

Now we have our discovery samples, we can simulate the resampling process of Marek et al. in order to compute statistical errors.


```{r Fast Resampling Simulator}
#Instead of resampling participants (as we skip simulating them here), we simulate data as if it had been resampled. Importantly, we use the fact that a resample adds sampling variability to the observed effect sizes in the discovery sample (full sample size).

#Distributions. In order to generate random data from the parametric null-distribution of a Pearson correlation on n=900 points, we want to pass a uniform distribution through an inverse cumulative distribution function.

#Initialise dataframes
Resample <- list()

  corrs <-array(0, c(nEffects, iter, length(binsize),length(power)))
  pvals <-array(0, c(nEffects, iter, length(binsize), length(power)))

  for(p in 1:length(power)){
      for(b in 1:length(binsize)){
        
        n <- binsize[b] #Target number of subjects, for simulating sampling variability
        
        df <- n-2

        ts <- c() #temporary list for sampled t statistics
        
        for(i in 1:iter){
          
        ts <- rt(nEffects, n-2) #Start by sampling all randomly (so we treat false-positives as random effects).
        
        corrs[,i,b,p] <-  ts/sqrt(n-2+ts^2) #Inverse function from Pearson r to t statistic.
        
        corrs[,i,b,p] <- corrs[,i,b,p]+Discovery$corrs[,p] #Add the discovery sample effects, as if resampled
        
        pvals[,i,b,p] <-  sapply(corrs[,i,b,p]*sqrt(df/(1-corrs[,i,b,p]^2)) , function(x){2*min(pt(x, df), pt(x, df, lower.tail=FALSE))}) #calculating p-values

      }
  }
}

Resample$corrs <- corrs
Resample$pvals <- pvals

```

Now we can compute the bias in statistical power. We first estimate power as in Marek et al., averaging over the 100 iterations, and then compare this to expected power.

```{r}

#initialise
estimatedPower <- array(0, dim=c(length(binsize),length(power))) 

for (a in 1:length(power)){ #Looping over our thresholds
    
    sigidx<-c() #initialise our significant effect index
    sigidx <- Discovery$pvals[,a]< thr #Find significant brain-behaviours in original sample according to our bonferroni corrected threshold.
   
      for (b in 1:length(binsize)){ #Looping over the resample size bins
        

estimatedPower[b,a] <- 1-sum(Resample$pvals[sigidx, ,b,a]>=thr)/(iter*sum(sigidx)) #1- false negative, the latter estimated by the proportion of originally significant edges which are non-significant in the resample, averaged over all iterations. Note this returns NaN when no edges pass the iteration threshold in the original sample.

}
}
```


```{r Power Figure}
colnames(estimatedPower) <- power

wrangle <- estimatedPower %>%
    as.tibble() %>% #tibble for ggplot 
    pivot_longer(cols = colnames(estimatedPower), names_to = 'expected', values_to='estimated') %>% #allows grouping by significance thresholds
    mutate(resize = rep(binsize[1:12], each=length(power)), expected=factor(expected, levels=power)) %>% #Add a column for resample size bins (our x-axis)
    group_by(expected) #group_by for distinct lines.
  

palette <- RColorBrewer::brewer.pal(11, "RdYlBu")


  out <- wrangle %>% ggplot() + 
  geom_line(aes(x=resize, y=estimated, colour=expected) ) + 
  theme_classic() + 
  scale_y_continuous(limits=c(0,1),breaks=c(0,0.1,0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9,1), labels=c('0%','10%','20%','30%','40%','50%', '60%', '70%', '80%','90%','100%'))+ 
  scale_x_continuous(breaks=binsize, trans='log10') + scale_fill_discrete(labels=thr) + labs(subtitle = 'Estimated Power (1-false negative rates)', y='Statistical Power', x='Sample Size', colour='\U003B1') +
   scale_colour_manual(values=palette, labels=c("0.01"," "," "," "," "," "," "," "," ","0.99"))+ 
  guides(colour = guide_coloursteps(even.steps=TRUE, show.limits=FALSE,direction= 'horizontal',reverse=0, barheight = unit(0.3, "cm"), barwidth=unit(2.1,"cm"),label.position='top', title.position="top", title="Discovery Power"))+theme(legend.position=c(0.17,0.35))
  
  out
```


```{r Analytic Power Curves}

#We perform power calculations for the true effect sizes across sample size (all bonferonni corrected)

analyticPower <- array(0,c(length(binsize),length(power)))

for(a in 1:length(power)){
  z <- atanh(rCrit[a]) #fisher transformation of effect size.
    Za <- qnorm(1-thr/2) #Critical Z score for bonferroni corrected threhshold
      for(b in 1:length(binsize)){
        n <- binsize[b]
        Zb <- sqrt(n-3)*z-Za      
        analyticPower[b,a] <- pnorm(Zb)
  }
}

colnames(analyticPower) <- power

analyticWrangle <- estimatedPower - analyticPower %>%
    as.tibble() 

analyticWrangle[,1] <- array(NaN,12)

analyticWrangle <- analyticWrangle %>%
#tibble for ggplot 
    pivot_longer(cols = colnames(analyticPower), names_to = 'expected', values_to='estimated') %>% #allows grouping by significance thresholds
    mutate(resize = rep(binsize[1:12], each=length(power)), expected=factor(expected, levels=power)) %>% #Add a column for resample size bins (our x-axis)
    group_by(expected) #group_by for distinct lines.
  

palette <- RColorBrewer::brewer.pal(11, "RdYlBu")


  BiasFigure <- analyticWrangle %>% ggplot() + 
  geom_line(aes(x=resize, y=estimated, colour=expected) ) + 
  theme_classic() + 
  scale_x_continuous(breaks=binsize, trans='log10') + scale_fill_discrete(labels=thr) + labs(subtitle = 'Bias (estimated - expected)', y='Statistical Power', x='Sample Size', colour='\U003B1') +
   scale_colour_manual(values=palette, labels=c("0.01"," "," "," "," "," "," "," "," ","0.99"))+ 
  guides(colour = guide_coloursteps(even.steps=TRUE, show.limits=FALSE,direction= 'horizontal',reverse=0, barheight = unit(0.3, "cm"), barwidth=unit(2.1,"cm"),label.position='top', title.position="top", title="Discovery Power"))+theme(legend.position=c(0.17,0.35))
  
 BiasFigure
```

