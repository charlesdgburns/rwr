---
title: "rwr_fastSim"
author: "Charles David Gutierrez Burns"
date: "04-09-2023"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
library(ggpubr)

set.seed(2022) #Setting the seed to the Marek et al. year for reproducibility.
knitr::opts_chunk$set(cache = TRUE) #Making the seed global so it applies to all chunks.
```

## Fast simulation

In a previous analysis (slow simulation) we simulated data per participant, correlating edges and behavioural score, and resampled participants in order to closely follow Marek, Tervo-Clemmens et al.

There we show strong bias in results when there is no effect, calling for further methodological investigation when true effects are present. We refer to the following simulations as 'fast' simulations, as we can simulate effects more efficiently now that we understand how sampling variability is compounded by resampling with replacement. Instead of painstakingly simulating non-gaussian multivariate data, we can simply draw from corresponding parametric distributions. 

This allows us to comprehensively answer 'when and how much' bias arises when resampling with replacement under various true effects.

The 'slow' simulations indicated clearly that uncorrected estimates are generally biased. 
This should not be surprising, as low statistical thresholds will include many false positives when not controlling for multiple comparisons. Furthermore, the 'slow' simulations also indicated that bias of resampling at the full sample size will depend on statistical power of the effect at the full sample size. For this reason, in the next analysis we fix the statistical threshold at Bonferroni corrected levels and vary statistical power.

We therefore aim to investigate a broad range of effect sizes, based on an inverse power calculation for a full sample size of n = 1,000 (as with our slow simulations).

As a final figure for these simulations, we want a graph for each statistical error which demonstrates bias (estimated - expected) for each statistical error for different levels of power at the full sample size. 

```{r initial parameters}
#### Input variables ####

#The parameters below may be scaled to lower computation time. Note, current code requires at least 16GB RAM. 

nDiscovery <- 1000; #sample size of our simulated data.
binsize <- c(25,33,50, 70, 100,135, 200, 265, 375, 525, 725, 1000); #resample size bins
iter <- 100 #Number of iterations
nEffects <- 55278 #100 choose 2; more effects leads to smoother bias estimations
threshold <- 0.05/nEffects #significance threshold, fixed here as power varies
power <- c(0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99)

```

We find critical effect sizes for a discovery sample of n = 1000, at a bonferroni corrected significance threshold.

```{r Analytic Power Calculations}
#First, we want to find the critical effect sizes for different levels of statistical power at the full sample size. 

rCrit <- c() #Initiating vector

for(p in 1:length(power)){
  Zb <- qnorm(power[p]) #Critical Z score for a given power
    Za <- qnorm(1-threshold/2) #Critical Z score for a given threshold
        z <- (Zb+Za)/sqrt(nDiscovery-3) #Power scaled to the distribution at full sample size
        rCrit[p] <- tanh(z) #Fisher transformation from Z dist to Pearson Correlation Dist.
  }

```

The above effect sizes will be used to simulate true effects of a given power level (at n=1000, alpha bonferonni corrected). 

Note that while real world effect sizes of a single BWAS may vary, we instead simulate several effects with the same underlying effect size. This should not be an issue, since the statistical error summary statistics are given as an average across effects, so in this simulation we can think of the underlying effects having an average effect size according to a given power level.

Below, we generate a discovery sample such that 50 of the effects are true effects, with their effect size scaled according to power levels. The number of true effects here is a somewhat arbitrary decision, although having more true effects will give a less noisy estimate of bias.

```{r Discovery Sample Generation}

nTrue <- 500 #round(nEffects/100); code here for 1% true effects, although only 12 effects lead to noise bias estimates.

nFalse <- nEffects - nTrue

Discovery <- list()

#initialise dataframes
Discovery$corrs <- array(0, dim=c(nEffects,length(power))) #generate a list of effects for each power
Discovery$pvals <- array(0, dim=c(nEffects,length(power))) #generate a list of pvalues for each power

for(p in 1:length(power)){

        zs <- c() #temporary hold some sampled z statistics
          
        zs <- rnorm(nEffects, 0, 1/sqrt(nDiscovery-3)) #Start by sampling all randomly.
        
        Discovery$corrs[,p] <-tanh(zs) #Fisher transformation from z to r for random effects.
        
        Discovery$corrs[1:nTrue,p] <- tanh(rnorm(nTrue, mean=atanh(rCrit[p]), 1/sqrt(nDiscovery-3))) #True effects simulated by fisher transformation, with mean atanh(rho) and sd 1/sqrt(n-3). These z scores are then transformed again into pearson correlations by tanh.
        
        df <- nDiscovery-2
        
        Discovery$pvals[,p] <-  sapply(Discovery$corrs[,p]*sqrt(df/(1-Discovery$corrs[,p]^2)) , function(x){2*min(pt(x, df), pt(x, df, lower.tail=FALSE))}) #calculating p-values using 'fast' t distribution.

}
```

For the sake of clarity, let's visualise these discovery samples. Note that our tricky task is to reliably estimate statistical errors while only having a single discovery sample. 

```{r Visualising discovery samples}

Samples <- Discovery$corrs

colnames(Samples) <- power

Tibble <- Samples %>% as_tibble() %>% mutate(idx = row_number(), real = case_when(idx <= nTrue ~ 1, idx >nTrue ~ 0) ) %>% pivot_longer(cols=colnames(Samples), names_to = "power", values_to= "r")  %>% group_by(power, real) %>% arrange(real)

DiscoveryPlot <- Tibble %>% ggplot(aes(x=factor(power), y=r))+
  scale_fill_manual(values=c("gray",'red'), labels=c("Random", "True")) + 
  geom_violin(aes(fill = factor(real)), width=0.4, position="identity", alpha=0.5)+
  geom_hline(yintercept=tanh((qnorm(1-threshold/2)/sqrt(nDiscovery-3))), linetype="dashed")+ 
  scale_shape_binned(solid=FALSE) + 
  labs(y= "Observed Effect Size (Pearson r)", x="Statistical Power (Ground truth)", subtitle=paste("Discovery Samples (",nEffects, " effects at N = ",nDiscovery,")", sep=""), fill="Effects") +
  theme_classic()+theme(legend.position="top")

DiscoveryPlot
```
In the figure above, red crosses are true effects while grey crosses are random; notice for 1% power these are practically indistinguishable from random effects. The dashed line represents the critical r for significance at Bonferroni correction level.

Now we have our discovery samples, we can simulate the resampling process of Marek et al. in order to compute statistical errors.


```{r Fast Resampling Simulator, include=FALSE}
#Instead of resampling participants (as we skip simulating them here), we simulate data as if it had been resampled. Importantly, we use the fact that a resample adds sampling variability to the observed effect sizes in the discovery sample (full sample size). Essentially, this treats observed effect sizes as true effects.

#Distributions. In order to generate random data from the parametric null-distribution of a Pearson correlation on n=900 points, we want to pass a uniform distribution through an inverse cumulative distribution function.

#Initialise dataframes
Resample <- list()

  corrs <-array(0, c(nEffects, iter, length(binsize),length(power)))
  pvals <-array(0, c(nEffects, iter, length(binsize), length(power)))

  for(p in 1:length(power)){
      for(b in 1:length(binsize)){
        
        n <- binsize[b] #Target number of subjects, for simulating sampling variabilities
        
        df <- n-2 #degrees of freedom used for calculating p-values
        
        zs <- c() #temporary list for sampled z statistics
        
        for(i in 1:iter){
          
        zs <- rnorm(nEffects, mean=atanh(Discovery$corrs[,p]), 1/sqrt(n-3)) #True effects simulated by fisher transformation, with mean being the observed p-value in the discovery and standard deviation 1/sqrt(n-3).

        
        corrs[,i,b,p] <- tanh(zs) #z scores are then transformed again into pearson correlations by tanh.
        
        pvals[,i,b,p] <-  sapply(corrs[,i,b,p]*sqrt(df/(1-corrs[,i,b,p]^2)) , function(x){2*min(pt(x, df), pt(x, df, lower.tail=FALSE))}) #calculating p-values using a 'fast' t-distribution method.

      }
  }
}

Resample$corrs <- corrs
Resample$pvals <- pvals

```

Now we can compute the bias in statistical power. We first estimate power as in Marek et al., averaging over the 1000 iterations, and then compare this to expected power.

```{r Estimated Power}

#initialise
estimatedPower <- array(0, dim=c(length(binsize),length(power))) 

for (a in 1:length(power)){ #Looping over our thresholds
    
    sigidx<-c() #initialise our significant effect index
    sigidx <- Discovery$pvals[,a]< threshold #Find significant brain-behaviours in original sample according to our bonferroni corrected threshold.
   
      for (b in 1:length(binsize)){ #Looping over the resample size bins
        

estimatedPower[b,a] <- 1-sum(Resample$pvals[sigidx, ,b,a]>=threshold)/(iter*sum(sigidx)) #1- false negative, the latter estimated by the proportion of originally significant edges which are non-significant in the resample, averaged over all iterations. Note this returns NaN when no edges pass the iteration threshold in the original sample.

}
}
```


```{r Power Figure}
colnames(estimatedPower) <- power

wrangle <- estimatedPower %>%
    as.tibble() %>% #tibble for ggplot 
    pivot_longer(cols = colnames(estimatedPower), names_to = 'expected', values_to='estimated') %>% #allows grouping by significance thresholds
    mutate(resize = rep(binsize[1:12], each=length(power)), expected=factor(expected, levels=power)) %>% #Add a column for resample size bins (our x-axis)
    group_by(expected) #group_by for distinct lines.
  

palette <- RColorBrewer::brewer.pal(11, "RdYlBu")


  PowerFigure <- wrangle %>% ggplot() + 
  geom_line(aes(x=resize, y=estimated, colour=expected) ) + 
  theme_minimal() + 
  scale_y_continuous(limits=c(0,1),breaks=c(0,0.1,0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9,1), labels=c('0%','10%','20%','30%','40%','50%', '60%', '70%', '80%','90%','100%'))+ 
  scale_x_continuous(breaks=binsize, trans='log10') + 
    scale_fill_discrete(labels=threshold) + 
    labs(subtitle='Estimate (Marek, Tervo-Clemmens et al.)',y='Statistical Power', x='Sample Size', colour='\U003B1') +
    scale_colour_manual(values=palette, labels=c("0.01"," "," "," "," "," "," "," "," ","0.99"))+ 
    guides(colour = guide_coloursteps(even.steps=TRUE, show.limits=FALSE,direction= 'horizontal',reverse=0, barheight = unit(0.3, "cm"), barwidth=unit(2.1,"cm"),label.position='top', title.position="top", title="Discovery Power"))+
    theme(legend.position=c(0.2,0.35))
  
  PowerFigure
```

While the above figure is useful for a reference

```{r Bias figure}

#We perform power calculations for the true effect sizes across sample size (all bonferonni corrected)

analyticPower <- array(0,c(length(binsize),length(power)))

for(a in 1:length(power)){
  z <- atanh(rCrit[a]) #fisher transformation of effect size.
    Za <- qnorm(1-threshold/2) #Critical Z score for bonferroni corrected threhshold
      for(b in 1:length(binsize)){
        n <- binsize[b]
        Zb <- sqrt(n-3)*z-Za      
        analyticPower[b,a] <- pnorm(Zb)
  }
}

colnames(analyticPower) <- power

analyticWrangle <- estimatedPower - analyticPower %>%
    as.tibble() 

analyticWrangle <- analyticWrangle %>%
#tibble for ggplot 
    pivot_longer(cols = colnames(analyticPower), names_to = 'expected', values_to='estimated') %>% #allows grouping by significance thresholds
    mutate(resize = rep(binsize[1:12], each=length(power)), expected=factor(expected, levels=power)) %>% #Add a column for resample size bins (our x-axis)
    group_by(expected) #group_by for distinct lines.
  

palette <- RColorBrewer::brewer.pal(11, "RdYlBu")


  BiasFigure <- analyticWrangle %>% 
    ggplot() + 
      geom_line(aes(x=resize, y=estimated, colour=expected) ) + 
      theme_minimal() + 
      scale_y_continuous(limits=c(-0.1,0.7), n.breaks=8)+
      scale_x_continuous(breaks=binsize, trans='log10') +
      scale_fill_discrete(labels=threshold) + labs(subtitle='Bias (estimated - expected)', y='Statistical Power', x='Sample Size', colour='\U003B1') +
    scale_colour_manual(values=palette, labels=c("0.01"," "," "," "," "," "," "," "," ","0.99"))+ 
    guides(colour = guide_coloursteps(even.steps=TRUE, show.limits=FALSE,direction= 'horizontal',reverse=0, barheight = unit(0.3, "cm"), barwidth=unit(2.1,"cm"),label.position='top', title.position="top", title="Discovery Power"))+
      theme(legend.position=c(0.2,0.35))
  
BiasFigure
```


```{r Figure}

Figure3 <- ggarrange(DiscoveryPlot, PowerFigure, BiasFigure,nrow = 1, labels=c("a","b","c"))

ggsave('figures/Figure3.png',
 (Figure3),bg='transparent',  width = 32, height = 8, dpi = 900, units = "cm", device='png')

Figure3
```

